{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import models,transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import csv\n",
    "import collections\n",
    "from  PIL import Image\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from scipy.spatial import distance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import math\n",
    "device = torch.device(\"mps\" if torch.has_mps else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### load data \n",
    "# create df to contain all identities, their image file names, their ethnicities\n",
    "path = \"data/RFW/images/test/txts/\"\n",
    "img_path = 'data/RFW/images/test/data/'\n",
    "\n",
    "# African images\n",
    "african_images = pd.read_csv(path + 'African/African_images.txt', sep=\"\\t\", header=None)\n",
    "african_images.columns = ['File', 'Label']\n",
    "african_images['identityID'] = african_images['File'].str[:-9]\n",
    "african_images['faceID'] = african_images['File'].str[-8:-4]\n",
    "african_images['Ethnicity'] = 'African'\n",
    "# Asian images\n",
    "asian_images = pd.read_csv(path + 'Asian/Asian_images.txt', sep=\"\\t\", header=None)\n",
    "asian_images.columns = ['File', 'Label']\n",
    "asian_images['identityID'] = asian_images['File'].str[:-9]\n",
    "asian_images['faceID'] = asian_images['File'].str[-8:-4]\n",
    "asian_images['Ethnicity'] = 'Asian'\n",
    "# Caucasian images\n",
    "caucasian_images = pd.read_csv(path + 'Caucasian/Caucasian_images.txt', sep=\"\\t\", header=None)\n",
    "caucasian_images.columns = ['File', 'Label']\n",
    "caucasian_images['identityID'] = caucasian_images['File'].str[:-9]\n",
    "caucasian_images['faceID'] = caucasian_images['File'].str[-8:-4]\n",
    "caucasian_images['Ethnicity'] = 'Caucasian'\n",
    "# Indian images\n",
    "indian_images = pd.read_csv(path + 'Indian/Indian_images.txt', sep=\"\\t\", header=None)\n",
    "indian_images.columns = ['File', 'Label']\n",
    "indian_images['identityID'] = indian_images['File'].str[:-9]\n",
    "indian_images['faceID'] = indian_images['File'].str[-8:-4]\n",
    "indian_images['Ethnicity'] = 'Indian'\n",
    "all_images = pd.concat([african_images,asian_images,caucasian_images,indian_images])\n",
    "\n",
    "# remove any duplicate identities\n",
    "v = all_images.reset_index().groupby('identityID').Ethnicity.nunique()\n",
    "dup = v[v>1].index.tolist()\n",
    "all_images = all_images[~all_images['identityID'].isin(dup)]\n",
    "\n",
    "# get first image from each identity and use it as reference\n",
    "identities = np.array(all_images.identityID.unique().tolist()).astype(object)\n",
    "file_end =  np.array('_0001.jpg'.split()*len(identities)).astype(object)\n",
    "first_images = identities + file_end\n",
    "\n",
    "references = all_images[all_images['File'].isin(first_images)]\n",
    "candidates = all_images[~all_images['File'].isin(first_images)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "african_references = references[references.Ethnicity=='African']\n",
    "african_candidates = candidates[candidates.Ethnicity=='African']\n",
    "caucasian_references = references[references.Ethnicity=='Caucasian']\n",
    "caucasian_candidates = candidates[candidates.Ethnicity=='Caucasian']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "__all__ = ['SENet', 'senet50']\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "# This SEModule is not used.\n",
    "class SEModule(nn.Module):\n",
    "\n",
    "    def __init__(self, planes, compress_rate):\n",
    "        super(SEModule, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(planes, planes // compress_rate, kernel_size=1, stride=1, bias=True)\n",
    "        self.conv2 = nn.Conv2d(planes // compress_rate, planes, kernel_size=1, stride=1, bias=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        module_input = x\n",
    "        x = F.avg_pool2d(module_input, kernel_size=module_input.size(2))\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return module_input * x\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "        # SENet\n",
    "        compress_rate = 16\n",
    "        # self.se_block = SEModule(planes * 4, compress_rate)  # this is not used.\n",
    "        self.conv4 = nn.Conv2d(planes * 4, planes * 4 // compress_rate, kernel_size=1, stride=1, bias=True)\n",
    "        self.conv5 = nn.Conv2d(planes * 4 // compress_rate, planes * 4, kernel_size=1, stride=1, bias=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "\n",
    "        ## senet\n",
    "        out2 = F.avg_pool2d(out, kernel_size=out.size(2))\n",
    "        out2 = self.conv4(out2)\n",
    "        out2 = self.relu(out2)\n",
    "        out2 = self.conv5(out2)\n",
    "        out2 = self.sigmoid(out2)\n",
    "        # out2 = self.se_block.forward(out)  # not used\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out = out2 * out + residual\n",
    "        # out = out2 + residual  # not used\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class SENet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=8631, include_top=False):\n",
    "        self.inplanes = 64\n",
    "        super(SENet, self).__init__()\n",
    "        self.include_top = include_top\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, ceil_mode=True)\n",
    "\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        \n",
    "        if not self.include_top:\n",
    "            return x\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def senet50(**kwargs):\n",
    "    \"\"\"Constructs a SENet-50 model.\n",
    "    \"\"\"\n",
    "    model = SENet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    return model\n",
    "\n",
    "# create dataset class for RFW\n",
    "class resnetRFW(data.Dataset):\n",
    "    \n",
    "    '''\n",
    "    This will be a class to load data from RFW for resnet50 model\n",
    "    '''\n",
    "     \n",
    "    mean_bgr = np.array([91.4953, 103.8827, 131.0912])  # from resnet50_ft.prototxt\n",
    "\n",
    "    def __init__(self,img_path,img_df):\n",
    "        \"\"\"\n",
    "        :param img_path: dataset directory\n",
    "        :param img_df: contains image file names and other information\n",
    "        \"\"\"\n",
    "        assert os.path.exists(img_path), \"root: {} not found.\".format(img_path)\n",
    "        self.img_path = img_path\n",
    "        self.img_df = img_df\n",
    "        self.img_info = []\n",
    "\n",
    "        for i, row in self.img_df.iterrows():\n",
    "            self.img_info.append({\n",
    "                'img_file': row.Ethnicity + '/' + row.identityID + '/' + row.File,\n",
    "                'identityID': row.identityID,\n",
    "                'Ethnicity': row.Ethnicity,\n",
    "                'faceID': row.faceID,\n",
    "            })\n",
    "            if i % 5000 == 0:\n",
    "                print(\"processing: {} images\".format(i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_info)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        info = self.img_info[index]\n",
    "        img_file = info['img_file']\n",
    "        img = Image.open(os.path.join(self.img_path, img_file))\n",
    "        img = transforms.Resize(256)(img)\n",
    "        img = transforms.CenterCrop(224)(img)\n",
    "        img = np.array(img, dtype=np.uint8)\n",
    "        assert len(img.shape) == 3  # assumes color images and no alpha channel\n",
    "\n",
    "        Ethnicity = info['Ethnicity']\n",
    "        identityID = info['identityID']\n",
    "        faceID = info['faceID']\n",
    "        return self.transform(img), identityID, Ethnicity, faceID\n",
    "  \n",
    "\n",
    "    def transform(self, img):\n",
    "        img = img[:, :, ::-1]  # RGB -> BGR\n",
    "        img = img.astype(np.float32)\n",
    "        img -= self.mean_bgr\n",
    "        img = img.transpose(2, 0, 1)  # C x H x W\n",
    "        img = torch.from_numpy(img).float()\n",
    "        return img\n",
    "\n",
    "    def untransform(self, img, lbl):\n",
    "        img = img.numpy()\n",
    "        img = img.transpose(1, 2, 0)\n",
    "        #img += self.mean_bgr\n",
    "        img = img.astype(np.uint8)\n",
    "        img = img[:, :, ::-1]\n",
    "        return img, lbl\n",
    "        \n",
    "def apply_model(model,dataloader,file_prefix,device):\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    identities = []\n",
    "    ethnicities = []\n",
    "    faceIDs = []\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, identityID, Ethnicity, faceID) in tqdm_notebook(enumerate(dataloader),total=len(dataloader)):\n",
    "            imgs = imgs.to(device)\n",
    "            x = model(imgs)\n",
    "            out = x.view(x.size(0),-1)\n",
    "            outputs.append(out)\n",
    "            identities.append(np.array(identityID))\n",
    "            ethnicities.append(np.array(Ethnicity))\n",
    "            faceIDs.append(np.array(faceID))\n",
    "\n",
    "    outputs=torch.cat(outputs)\n",
    "    identities= np.concatenate(np.array(identities)).ravel()\n",
    "    ethnicities= np.concatenate(np.array(ethnicities)).ravel()\n",
    "    faceIDs= np.concatenate(np.array(faceIDs)).ravel()\n",
    "\n",
    "    torch.save(outputs, file_prefix + '_outputs.pt')\n",
    "    np.save(file_prefix + '_identities.npy', identities)\n",
    "    np.save(file_prefix + '_ethnicities.npy', ethnicities)\n",
    "    np.save(file_prefix + '_faceIDs.npy', faceIDs)\n",
    "    return outputs, identities, ethnicities, faceIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and assign weights\n",
    "ft_weights=\"weights/senet50_ft_weight.pkl\"\n",
    "senet50_ft = senet50()\n",
    "\n",
    "with open(ft_weights, 'rb') as f:\n",
    "    weights = pickle.load(f, encoding='latin1')\n",
    "\n",
    "own_state = senet50_ft.state_dict()\n",
    "for name, param in weights.items():\n",
    "    if name in own_state:\n",
    "        try:\n",
    "            own_state[name].copy_(torch.from_numpy(param))\n",
    "        except Exception:\n",
    "            raise RuntimeError('While copying the parameter named {}, whose dimensions in the model are {} and whose '\\\n",
    "                                'dimensions in the checkpoint are {}.'.format(name, own_state[name].size(), param.shape))\n",
    "    else:\n",
    "        raise KeyError('unexpected key \"{}\" in state_dict'.format(name))\n",
    "\n",
    "model_ft = senet50_ft.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: 0 images\n",
      "processing: 0 images\n",
      "processing: 5000 images\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'num_workers': 4, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "# load reference images\n",
    "african_reference_dataset = resnetRFW(img_path,african_references.reset_index(drop=True))\n",
    "african_reference_loader = torch.utils.data.DataLoader(african_reference_dataset, batch_size=4, shuffle=False, **kwargs)\n",
    "# load candidate images\n",
    "african_candidate_dataset = resnetRFW(img_path,african_candidates.reset_index(drop=True))\n",
    "african_candidate_loader = torch.utils.data.DataLoader(african_candidate_dataset, batch_size=4, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e67a23aca17e4d34aa54fd6c6b1a0451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/746 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066b32184be14612ab4da9c0a52f1a2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1849 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "african_reference_outputs, african_reference_identities, african_reference_ethnicities, african_reference_faceIDs = apply_model(model_ft,african_reference_loader,'outputs/RFW/ft/reference2',device)\n",
    "african_candidate_outputs, african_candidate_identities, african_candidate_ethnicities, african_candidate_faceIDs = apply_model(model_ft,african_candidate_loader,'outputs/RFW/ft/candidate2',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr2_coeff(A, B):\n",
    "    # Rowwise mean of input arrays & subtract from input arrays themeselves\n",
    "    A_mA = A - A.mean(1)[:, None]\n",
    "    B_mB = B - B.mean(1)[:, None]\n",
    "\n",
    "    # Sum of squares across rows\n",
    "    ssA = (A_mA**2).sum(1)\n",
    "    ssB = (B_mB**2).sum(1)\n",
    "\n",
    "    # Finally get corr coeff\n",
    "    return torch.matmul(A_mA, B_mB.T) / torch.sqrt(torch.matmul(ssA[:, None],ssB[None]))\n",
    "def cos_sim(a, b, eps=1e-8):\n",
    "    \"\"\"\n",
    "    added eps for numerical stability\n",
    "    \"\"\"\n",
    "    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "    a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
    "    b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
    "    sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "    return sim_mt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = corr2_coeff(african_reference_outputs,african_candidate_outputs).cpu().detach().numpy()\n",
    "cos = cos_sim(african_reference_outputs,african_candidate_outputs).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verification(similarity_mat, thresh,reference_identities,reference_ethnicities,candidate_identities):\n",
    "    verification = pd.DataFrame(columns=['reference_identity','reference_ethnicity','TPR','TNR','FPR','FNR'])\n",
    "    match_mat = np.abs(similarity_mat)>=thresh\n",
    "    for i, match_row in tqdm_notebook(enumerate(match_mat),total=len(match_mat)):\n",
    "        identity = reference_identities[i]\n",
    "        ethnicity = reference_ethnicities[i]\n",
    "        matches = candidate_identities[match_row]  \n",
    "        not_matches = candidate_identities[~match_row]\n",
    "        TP = np.sum(matches == identity)\n",
    "        FP = np.sum(matches != identity)\n",
    "        TN = np.sum(not_matches != identity)\n",
    "        FN = np.sum(not_matches == identity)\n",
    "        TPR = TP/(TP+FN)\n",
    "        TNR = TN/(TN+FP)\n",
    "        FPR = FP/(TN+FP)\n",
    "        FNR = FN/(TP+FN)\n",
    "        row = {'reference_identity': identity,\n",
    "           'reference_ethnicity': ethnicity, \n",
    "           'TPR': TPR,\n",
    "           'TNR': TNR,\n",
    "           'FPR': FPR,\n",
    "           'FNR': FNR}\n",
    "        verification = verification.append(row,ignore_index=True)\n",
    "    return verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999e832f9f3246cb83350b22c5aba387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2982 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 0.59 TPR = 0.8486\n",
      "threshold = 0.59 FPR = 0.0118\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9dc2c89af0d4c869513c3b7c3e44684",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2982 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 0.5925 TPR = 0.8432\n",
      "threshold = 0.5925 FPR = 0.0111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e236acc85e84526a74055978d775702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2982 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 0.595 TPR = 0.8377\n",
      "threshold = 0.595 FPR = 0.0104\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a69fc8650ac489096eb4cee818fdbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2982 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 0.5975 TPR = 0.8326\n",
      "threshold = 0.5975 FPR = 0.0097\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50815156650143c6a06147702f394a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2982 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 0.6 TPR = 0.829\n",
      "threshold = 0.6 FPR = 0.0091\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dcb426293e449ef855b873e8c10fc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2982 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000014?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m thresh \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marange(\u001b[39m0.59\u001b[39m,\u001b[39m0.61\u001b[39m,\u001b[39m0.0025\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000014?line=1'>2</a>\u001b[0m     cos_verificaiton \u001b[39m=\u001b[39m verification(cos, thresh,african_reference_identities,african_reference_ethnicities,african_candidate_identities)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000014?line=2'>3</a>\u001b[0m     TPR \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mround(cos_verificaiton\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mTPR\u001b[39m\u001b[39m'\u001b[39m],\u001b[39m4\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000014?line=3'>4</a>\u001b[0m     FPR \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mround(cos_verificaiton\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mFPR\u001b[39m\u001b[39m'\u001b[39m],\u001b[39m4\u001b[39m)\n",
      "\u001b[1;32m/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb Cell 10'\u001b[0m in \u001b[0;36mverification\u001b[0;34m(similarity_mat, thresh, reference_identities, reference_ethnicities, candidate_identities)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=15'>16</a>\u001b[0m     FNR \u001b[39m=\u001b[39m FN\u001b[39m/\u001b[39m(TP\u001b[39m+\u001b[39mFN)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=16'>17</a>\u001b[0m     row \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mreference_identity\u001b[39m\u001b[39m'\u001b[39m: identity,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=17'>18</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mreference_ethnicity\u001b[39m\u001b[39m'\u001b[39m: ethnicity, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=18'>19</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mTPR\u001b[39m\u001b[39m'\u001b[39m: TPR,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=19'>20</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mTNR\u001b[39m\u001b[39m'\u001b[39m: TNR,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=20'>21</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mFPR\u001b[39m\u001b[39m'\u001b[39m: FPR,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=21'>22</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mFNR\u001b[39m\u001b[39m'\u001b[39m: FNR}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=22'>23</a>\u001b[0m     verification \u001b[39m=\u001b[39m verification\u001b[39m.\u001b[39;49mappend(row,ignore_index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=23'>24</a>\u001b[0m \u001b[39mreturn\u001b[39;00m verification\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mpsEnv/lib/python3.9/site-packages/pandas/core/frame.py:9039\u001b[0m, in \u001b[0;36mDataFrame.append\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   8936\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   8937\u001b[0m \u001b[39mAppend rows of `other` to the end of caller, returning a new object.\u001b[39;00m\n\u001b[1;32m   8938\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9029\u001b[0m \u001b[39m4  4\u001b[39;00m\n\u001b[1;32m   9030\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   9031\u001b[0m warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   9032\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mThe frame.append method is deprecated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   9033\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mand will be removed from pandas in a future version. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9036\u001b[0m     stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   9037\u001b[0m )\n\u001b[0;32m-> 9039\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_append(other, ignore_index, verify_integrity, sort)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mpsEnv/lib/python3.9/site-packages/pandas/core/frame.py:9082\u001b[0m, in \u001b[0;36mDataFrame._append\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   9079\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   9080\u001b[0m     to_concat \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m, other]\n\u001b[0;32m-> 9082\u001b[0m result \u001b[39m=\u001b[39m concat(\n\u001b[1;32m   9083\u001b[0m     to_concat,\n\u001b[1;32m   9084\u001b[0m     ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m   9085\u001b[0m     verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m   9086\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m   9087\u001b[0m )\n\u001b[1;32m   9088\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   9089\u001b[0m     combined_columns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   9090\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sort\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9095\u001b[0m     \u001b[39m# combined_columns.equals check is necessary for preserving dtype\u001b[39;00m\n\u001b[1;32m   9096\u001b[0m     \u001b[39m#  in test_crosstab_normalize\u001b[39;00m\n\u001b[1;32m   9097\u001b[0m     result \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mreindex(combined_columns, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mpsEnv/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mpsEnv/lib/python3.9/site-packages/pandas/core/reshape/concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[1;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m    156\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[39m    along the other axes.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[39m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    348\u001b[0m         objs,\n\u001b[1;32m    349\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    350\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    351\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    352\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    353\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    354\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    355\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    356\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    357\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    358\u001b[0m     )\n\u001b[1;32m    360\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mpsEnv/lib/python3.9/site-packages/pandas/core/reshape/concat.py:542\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverify_integrity \u001b[39m=\u001b[39m verify_integrity\n\u001b[1;32m    540\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39m=\u001b[39m copy\n\u001b[0;32m--> 542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnew_axes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_new_axes()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mpsEnv/lib/python3.9/site-packages/pandas/core/reshape/concat.py:612\u001b[0m, in \u001b[0;36m_Concatenator._get_new_axes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_new_axes\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Index]:\n\u001b[1;32m    611\u001b[0m     ndim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_result_dim()\n\u001b[0;32m--> 612\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m    613\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_concat_axis \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbm_axis \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_comb_axis(i)\n\u001b[1;32m    614\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ndim)\n\u001b[1;32m    615\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mpsEnv/lib/python3.9/site-packages/pandas/core/reshape/concat.py:613\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_new_axes\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mlist\u001b[39m[Index]:\n\u001b[1;32m    611\u001b[0m     ndim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_result_dim()\n\u001b[1;32m    612\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m--> 613\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_concat_axis \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbm_axis \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_comb_axis(i)\n\u001b[1;32m    614\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(ndim)\n\u001b[1;32m    615\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mpsEnv/lib/python3.9/site-packages/pandas/core/reshape/concat.py:618\u001b[0m, in \u001b[0;36m_Concatenator._get_comb_axis\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_comb_axis\u001b[39m(\u001b[39mself\u001b[39m, i: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Index:\n\u001b[0;32m--> 618\u001b[0m     data_axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobjs[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49m_get_block_manager_axis(i)\n\u001b[1;32m    619\u001b[0m     \u001b[39mreturn\u001b[39;00m get_objs_combined_axis(\n\u001b[1;32m    620\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjs,\n\u001b[1;32m    621\u001b[0m         axis\u001b[39m=\u001b[39mdata_axis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         copy\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy,\n\u001b[1;32m    625\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for thresh in np.arange(0.59,0.61,0.0025):\n",
    "    cos_verificaiton = verification(cos, thresh,african_reference_identities,african_reference_ethnicities,african_candidate_identities)\n",
    "    TPR = np.round(cos_verificaiton.mean(axis=0)['TPR'],4)\n",
    "    FPR = np.round(cos_verificaiton.mean(axis=0)['FPR'],4)\n",
    "\n",
    "\n",
    "    print('threshold =', np.round(thresh,4), 'TPR =', TPR)\n",
    "    print('threshold =', np.round(thresh,4), 'FPR =', FPR)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: 0 images\n",
      "processing: 0 images\n",
      "processing: 5000 images\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc6d2dcc724453bb81f72095a95b84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/740 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771bd47193e649d08d4f6c7ee7431a64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1810 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform on caucasian images\n",
    "kwargs = {'num_workers': 4, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "# load reference images\n",
    "caucasian_reference_dataset = resnetRFW(img_path,caucasian_references.reset_index(drop=True))\n",
    "caucasian_reference_loader = torch.utils.data.DataLoader(caucasian_reference_dataset, batch_size=4, shuffle=False, **kwargs)\n",
    "# load candidate images\n",
    "caucasian_candidate_dataset = resnetRFW(img_path,caucasian_candidates.reset_index(drop=True))\n",
    "caucasian_candidate_loader = torch.utils.data.DataLoader(caucasian_candidate_dataset, batch_size=4, shuffle=False, **kwargs)\n",
    "\n",
    "caucasian_reference_outputs, caucasian_reference_identities, caucasian_reference_ethnicities, caucasian_reference_faceIDs = apply_model(model_ft,caucasian_reference_loader,'outputs/RFW/ft/reference2_caucasian',device)\n",
    "caucasian_candidate_outputs, caucasian_candidate_identities, caucasian_candidate_ethnicities, caucasian_candidate_faceIDs = apply_model(model_ft,caucasian_candidate_loader,'outputs/RFW/ft/candidate2_caucasian',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "caucasian_cor = corr2_coeff(caucasian_reference_outputs,caucasian_candidate_outputs).cpu().detach().numpy()\n",
    "caucasian_cos = cos_sim(caucasian_reference_outputs,caucasian_candidate_outputs).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab1a39a090446259ea48e52df2e1105",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 0.475 TPR = 0.9426\n",
      "threshold = 0.475 FPR = 0.0137\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10afa21f17d74339a1a450bd50f9b9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 0.4775 TPR = 0.9399\n",
      "threshold = 0.4775 FPR = 0.0129\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2aaabc2a0894f11b98280a1cff8cb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 0.48 TPR = 0.9376\n",
      "threshold = 0.48 FPR = 0.0122\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd37bc7dfda4271962a7095919f895b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 0.4825 TPR = 0.9357\n",
      "threshold = 0.4825 FPR = 0.0115\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c12502c012904fbfa2f0974b15bb58c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 0.485 TPR = 0.933\n",
      "threshold = 0.485 FPR = 0.0108\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c175a75b84b94bbb8b42ba9dbd3af0b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 0.4875 TPR = 0.93\n",
      "threshold = 0.4875 FPR = 0.0102\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1524a590464de1932ca344cbe014b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold = 0.49 TPR = 0.9277\n",
      "threshold = 0.49 FPR = 0.0095\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63146b0925974ecc883de63ef686bbe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2958 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000017?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m thresh \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marange(\u001b[39m0.475\u001b[39m,\u001b[39m0.55\u001b[39m,\u001b[39m0.0025\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000017?line=1'>2</a>\u001b[0m     cos_verificaiton \u001b[39m=\u001b[39m verification(caucasian_cos, thresh,caucasian_reference_identities,caucasian_reference_ethnicities,caucasian_candidate_identities)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000017?line=2'>3</a>\u001b[0m     TPR \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mround(cos_verificaiton\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mTPR\u001b[39m\u001b[39m'\u001b[39m],\u001b[39m4\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000017?line=3'>4</a>\u001b[0m     FPR \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mround(cos_verificaiton\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39mFPR\u001b[39m\u001b[39m'\u001b[39m],\u001b[39m4\u001b[39m)\n",
      "\u001b[1;32m/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb Cell 10'\u001b[0m in \u001b[0;36mverification\u001b[0;34m(similarity_mat, thresh, reference_identities, reference_ethnicities, candidate_identities)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=15'>16</a>\u001b[0m     FNR \u001b[39m=\u001b[39m FN\u001b[39m/\u001b[39m(TP\u001b[39m+\u001b[39mFN)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=16'>17</a>\u001b[0m     row \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mreference_identity\u001b[39m\u001b[39m'\u001b[39m: identity,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=17'>18</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mreference_ethnicity\u001b[39m\u001b[39m'\u001b[39m: ethnicity, \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=18'>19</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mTPR\u001b[39m\u001b[39m'\u001b[39m: TPR,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=19'>20</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mTNR\u001b[39m\u001b[39m'\u001b[39m: TNR,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=20'>21</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mFPR\u001b[39m\u001b[39m'\u001b[39m: FPR,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=21'>22</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mFNR\u001b[39m\u001b[39m'\u001b[39m: FNR}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=22'>23</a>\u001b[0m     verification \u001b[39m=\u001b[39m verification\u001b[39m.\u001b[39;49mappend(row,ignore_index\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sachinbeepath/HolisticAI/DISS_CODE/MSc_Dissertation/SENettest.ipynb#ch0000013?line=23'>24</a>\u001b[0m \u001b[39mreturn\u001b[39;00m verification\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mpsEnv/lib/python3.9/site-packages/pandas/core/frame.py:9036\u001b[0m, in \u001b[0;36mDataFrame.append\u001b[0;34m(self, other, ignore_index, verify_integrity, sort)\u001b[0m\n\u001b[1;32m   8929\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mappend\u001b[39m(\n\u001b[1;32m   8930\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   8931\u001b[0m     other,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8934\u001b[0m     sort: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   8935\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m   8936\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   8937\u001b[0m \u001b[39m    Append rows of `other` to the end of caller, returning a new object.\u001b[39;00m\n\u001b[1;32m   8938\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9029\u001b[0m \u001b[39m    4  4\u001b[39;00m\n\u001b[1;32m   9030\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   9031\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   9032\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe frame.append method is deprecated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   9033\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mand will be removed from pandas in a future version. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   9034\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUse pandas.concat instead.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   9035\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m-> 9036\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m   9037\u001b[0m     )\n\u001b[1;32m   9039\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_append(other, ignore_index, verify_integrity, sort)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mpsEnv/lib/python3.9/site-packages/pandas/util/_exceptions.py:32\u001b[0m, in \u001b[0;36mfind_stack_level\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfind_stack_level\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m     28\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m    Find the first place in the stack that is not inside pandas\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39m    (tests notwithstanding).\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m     stack \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39;49mstack()\n\u001b[1;32m     34\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     pkg_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(pd\u001b[39m.\u001b[39m\u001b[39m__file__\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mpsEnv/lib/python3.9/inspect.py:1554\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstack\u001b[39m(context\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1553\u001b[0m     \u001b[39m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1554\u001b[0m     \u001b[39mreturn\u001b[39;00m getouterframes(sys\u001b[39m.\u001b[39;49m_getframe(\u001b[39m1\u001b[39;49m), context)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mpsEnv/lib/python3.9/inspect.py:1531\u001b[0m, in \u001b[0;36mgetouterframes\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1529\u001b[0m framelist \u001b[39m=\u001b[39m []\n\u001b[1;32m   1530\u001b[0m \u001b[39mwhile\u001b[39;00m frame:\n\u001b[0;32m-> 1531\u001b[0m     frameinfo \u001b[39m=\u001b[39m (frame,) \u001b[39m+\u001b[39m getframeinfo(frame, context)\n\u001b[1;32m   1532\u001b[0m     framelist\u001b[39m.\u001b[39mappend(FrameInfo(\u001b[39m*\u001b[39mframeinfo))\n\u001b[1;32m   1533\u001b[0m     frame \u001b[39m=\u001b[39m frame\u001b[39m.\u001b[39mf_back\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mpsEnv/lib/python3.9/inspect.py:1501\u001b[0m, in \u001b[0;36mgetframeinfo\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m isframe(frame):\n\u001b[1;32m   1499\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m is not a frame or traceback object\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(frame))\n\u001b[0;32m-> 1501\u001b[0m filename \u001b[39m=\u001b[39m getsourcefile(frame) \u001b[39mor\u001b[39;00m getfile(frame)\n\u001b[1;32m   1502\u001b[0m \u001b[39mif\u001b[39;00m context \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1503\u001b[0m     start \u001b[39m=\u001b[39m lineno \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m context\u001b[39m/\u001b[39m\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mpsEnv/lib/python3.9/inspect.py:709\u001b[0m, in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[39mreturn\u001b[39;00m filename\n\u001b[1;32m    708\u001b[0m \u001b[39m# only return a non-existent filename if the module has a PEP 302 loader\u001b[39;00m\n\u001b[0;32m--> 709\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(getmodule(\u001b[39mobject\u001b[39;49m, filename), \u001b[39m'\u001b[39m\u001b[39m__loader__\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    710\u001b[0m     \u001b[39mreturn\u001b[39;00m filename\n\u001b[1;32m    711\u001b[0m \u001b[39m# or it is in the linecache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/mpsEnv/lib/python3.9/inspect.py:746\u001b[0m, in \u001b[0;36mgetmodule\u001b[0;34m(object, _filename)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39m# Update the filename to module name cache and check yet again\u001b[39;00m\n\u001b[1;32m    744\u001b[0m \u001b[39m# Copy sys.modules in order to cope with changes while iterating\u001b[39;00m\n\u001b[1;32m    745\u001b[0m \u001b[39mfor\u001b[39;00m modname, module \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mmodules\u001b[39m.\u001b[39mcopy()\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 746\u001b[0m     \u001b[39mif\u001b[39;00m ismodule(module) \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(module, \u001b[39m'\u001b[39m\u001b[39m__file__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    747\u001b[0m         f \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m\u001b[39m__file__\u001b[39m\n\u001b[1;32m    748\u001b[0m         \u001b[39mif\u001b[39;00m f \u001b[39m==\u001b[39m _filesbymodname\u001b[39m.\u001b[39mget(modname, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    749\u001b[0m             \u001b[39m# Have already mapped this module, so skip it\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for thresh in np.arange(0.475,0.55,0.0025):\n",
    "    cos_verificaiton = verification(caucasian_cos, thresh,caucasian_reference_identities,caucasian_reference_ethnicities,caucasian_candidate_identities)\n",
    "    TPR = np.round(cos_verificaiton.mean(axis=0)['TPR'],4)\n",
    "    FPR = np.round(cos_verificaiton.mean(axis=0)['FPR'],4)\n",
    "\n",
    "\n",
    "    print('threshold =', np.round(thresh,4), 'TPR =', TPR)\n",
    "    print('threshold =', np.round(thresh,4), 'FPR =', FPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mpsEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dd368a5ef6c8892f6c87c22c8d56888e698af74589d34ad1bd17c3290e3a6a05"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
