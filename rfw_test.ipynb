{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring RFW data\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import models,transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from collections import OrderedDict\n",
    "import csv\n",
    "import collections\n",
    "from  PIL import Image\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from scipy.spatial import distance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df to contain all identities, their image file names, their ethnicities\n",
    "path = \"data/RFW/images/test/txts/\"\n",
    "img_path = 'data/RFW/images/test/data/'\n",
    "\n",
    "# African images\n",
    "african_images = pd.read_csv(path + 'African/African_images.txt', sep=\"\\t\", header=None)\n",
    "african_images.columns = ['File', 'Label']\n",
    "african_images['identityID'] = african_images['File'].str[:-9]\n",
    "african_images['faceID'] = african_images['File'].str[-8:-4]\n",
    "african_images['Ethnicity'] = 'African'\n",
    "# Asian images\n",
    "asian_images = pd.read_csv(path + 'Asian/Asian_images.txt', sep=\"\\t\", header=None)\n",
    "asian_images.columns = ['File', 'Label']\n",
    "asian_images['identityID'] = asian_images['File'].str[:-9]\n",
    "asian_images['faceID'] = asian_images['File'].str[-8:-4]\n",
    "asian_images['Ethnicity'] = 'Asian'\n",
    "# Caucasian images\n",
    "caucasian_images = pd.read_csv(path + 'Caucasian/Caucasian_images.txt', sep=\"\\t\", header=None)\n",
    "caucasian_images.columns = ['File', 'Label']\n",
    "caucasian_images['identityID'] = caucasian_images['File'].str[:-9]\n",
    "caucasian_images['faceID'] = caucasian_images['File'].str[-8:-4]\n",
    "caucasian_images['Ethnicity'] = 'Caucasian'\n",
    "# Indian images\n",
    "indian_images = pd.read_csv(path + 'Indian/Indian_images.txt', sep=\"\\t\", header=None)\n",
    "indian_images.columns = ['File', 'Label']\n",
    "indian_images['identityID'] = indian_images['File'].str[:-9]\n",
    "indian_images['faceID'] = indian_images['File'].str[-8:-4]\n",
    "indian_images['Ethnicity'] = 'Indian'\n",
    "all_images = pd.concat([african_images,asian_images,caucasian_images,indian_images])\n",
    "all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove any duplicate identities\n",
    "v = all_images.reset_index().groupby('identityID').Ethnicity.nunique()\n",
    "dup = v[v>1].index.tolist()\n",
    "all_images = all_images[~all_images['identityID'].isin(dup)]\n",
    "all_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first image from each identity and use it as reference\n",
    "identities = np.array(all_images.identityID.unique().tolist()).astype(object)\n",
    "file_end =  np.array('_0001.jpg'.split()*len(identities)).astype(object)\n",
    "first_images = identities + file_end\n",
    "first_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "references = all_images[all_images['File'].isin(first_images)]\n",
    "candidates = all_images[~all_images['File'].isin(first_images)]\n",
    "print(len(references),len(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset class for RFW\n",
    "class resnetRFW(data.Dataset):\n",
    "    \n",
    "    '''\n",
    "    This will be a class to load data from RFW for resnet50 model\n",
    "    '''\n",
    "     \n",
    "    mean_bgr = np.array([91.4953, 103.8827, 131.0912])  # from resnet50_ft.prototxt\n",
    "\n",
    "    def __init__(self,img_path,img_df):\n",
    "        \"\"\"\n",
    "        :param img_path: dataset directory\n",
    "        :param img_df: contains image file names and other information\n",
    "        \"\"\"\n",
    "        assert os.path.exists(img_path), \"root: {} not found.\".format(img_path)\n",
    "        self.img_path = img_path\n",
    "        self.img_df = img_df\n",
    "        self.img_info = []\n",
    "\n",
    "        for i, row in self.img_df.iterrows():\n",
    "            self.img_info.append({\n",
    "                'img_file': row.Ethnicity + '/' + row.identityID + '/' + row.File,\n",
    "                'identityID': row.identityID,\n",
    "                'Ethnicity': row.Ethnicity,\n",
    "                'faceID': row.faceID,\n",
    "            })\n",
    "            if i % 5000 == 0:\n",
    "                print(\"processing: {} images\".format(i))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_info)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        info = self.img_info[index]\n",
    "        img_file = info['img_file']\n",
    "        img = Image.open(os.path.join(self.img_path, img_file))\n",
    "        img = transforms.Resize(256)(img)\n",
    "        img = transforms.CenterCrop(224)(img)\n",
    "        img = np.array(img, dtype=np.uint8)\n",
    "        assert len(img.shape) == 3  # assumes color images and no alpha channel\n",
    "\n",
    "        Ethnicity = info['Ethnicity']\n",
    "        identityID = info['identityID']\n",
    "        faceID = info['faceID']\n",
    "        return self.transform(img), identityID, Ethnicity, faceID\n",
    "  \n",
    "\n",
    "    def transform(self, img):\n",
    "        img = img[:, :, ::-1]  # RGB -> BGR\n",
    "        img = img.astype(np.float32)\n",
    "        img -= self.mean_bgr\n",
    "        img = img.transpose(2, 0, 1)  # C x H x W\n",
    "        img = torch.from_numpy(img).float()\n",
    "        return img\n",
    "\n",
    "    def untransform(self, img, lbl):\n",
    "        img = img.numpy()\n",
    "        img = img.transpose(1, 2, 0)\n",
    "        img += self.mean_bgr\n",
    "        img = img.astype(np.uint8)\n",
    "        img = img[:, :, ::-1]\n",
    "        return img, lbl\n",
    "\n",
    "def load_resnet50(weights=\"weights/resnet50_scratch_weight.pkl\"):\n",
    "    # load resnet50 model and modify it to match the one from the github to load the weights from the pkl\n",
    "# resnet50 trained on VGGFace2\n",
    "    resnet50 = models.resnet50(pretrained=False)\n",
    "    resnet50.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
    "    resnet50.layer2[0].conv1 = nn.Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "    resnet50.layer2[0].conv2 = nn.Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    resnet50.layer3[0].conv1 = nn.Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "    resnet50.layer3[0].conv2 = nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    resnet50.layer4[0].conv1 = nn.Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
    "    resnet50.layer4[0].conv2 = nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    resnet50.avgpool = nn.AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
    "    resnet50.fc = nn.Linear(in_features=2048,out_features=8631)\n",
    "    with open(\"weights/resnet50_scratch_weight.pkl\", 'rb') as f:\n",
    "        weights = pickle.load(f, encoding='latin1')\n",
    "    weights = dict(map(lambda x: (x[0], torch.from_numpy(x[1])), weights.items()))\n",
    "    weights = OrderedDict(weights)\n",
    "    resnet50.load_state_dict(weights)\n",
    "    resnet50 = torch.nn.Sequential(*(list(resnet50.children())[:-1]))\n",
    "    return resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 4, 'pin_memory': True} if torch.cuda.is_available() else {}\n",
    "# load reference images\n",
    "reference_dataset = resnetRFW(img_path,references.reset_index(drop=True))\n",
    "reference_loader = torch.utils.data.DataLoader(reference_dataset, batch_size=32, shuffle=False, **kwargs)\n",
    "# load candidate images\n",
    "candidate_dataset = resnetRFW(img_path,candidates.reset_index(drop=True))\n",
    "candidate_loader = torch.utils.data.DataLoader(candidate_dataset, batch_size=32, shuffle=False, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and assign weights\n",
    "resnet50 = load_resnet50()\n",
    "resnet50.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(model,dataloader,file_prefix):\n",
    "\n",
    "    outputs = []\n",
    "    identities = []\n",
    "    ethnicities = []\n",
    "    faceIDs = []\n",
    "    with torch.no_grad():\n",
    "        for i, (imgs, identityID, Ethnicity, faceID) in tqdm_notebook(enumerate(dataloader),total=len(dataloader)):\n",
    "            x = model(imgs)\n",
    "            out = x.view(x.size(0),-1)\n",
    "            outputs.append(out)\n",
    "            identities.append(np.array(identityID))\n",
    "            ethnicities.append(np.array(Ethnicity))\n",
    "            faceIDs.append(np.array(faceID))\n",
    "\n",
    "    outputs=torch.cat(outputs)\n",
    "    identities= np.concatenate(np.array(identities)).ravel()\n",
    "    ethnicities= np.concatenate(np.array(ethnicities)).ravel()\n",
    "    faceIDs= np.concatenate(np.array(faceIDs)).ravel()\n",
    "\n",
    "    torch.save(outputs, file_prefix + '_outputs.pt')\n",
    "    np.save(file_prefix + '_identities.npy', identities)\n",
    "    np.save(file_prefix + '_ethnicities.npy', ethnicities)\n",
    "    np.save(file_prefix + '_faceIDs.npy', faceIDs)\n",
    "    return outputs, identities, ethnicities, faceIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply model to references\n",
    "\n",
    "reference_outputs, reference_identities, reference_ethnicities, reference_faceIDs = apply_model(resnet50,candidate_loader,'reference')\n",
    "\"\"\" outputs = []\n",
    "identities = []\n",
    "ethnicities = []\n",
    "faceIDs = []\n",
    "with torch.no_grad():\n",
    "    for i, (imgs, identityID, Ethnicity, faceID) in tqdm_notebook(enumerate(reference_loader),total=len(reference_loader)):\n",
    "        x = resnet50(imgs)\n",
    "        out = x.view(x.size(0),-1)\n",
    "        outputs.append(out)\n",
    "        identities.append(np.array(identityID))\n",
    "        ethnicities.append(np.array(Ethnicity))\n",
    "        faceIDs.append(np.array(faceID))\n",
    "\n",
    "outputs=torch.cat(outputs)\n",
    "identities= np.array(identities)\n",
    "ethnicities= np.array(ethnicities)\n",
    "faceIDs= np.array(faceIDs)\n",
    "\n",
    "torch.save(outputs, 'reference_outputs.pt')\n",
    "np.save('reference_identities.npy', identities)\n",
    "np.save('reference_ethnicities.npy', ethnicities)\n",
    "np.save('reference_faceIDs.npy', faceIDs) \"\"\"\n",
    "\n",
    "# apply model to candidates\n",
    "\n",
    "candidate_outputs, candidate_identities, candidate_ethnicities, candidate_faceIDs = apply_model(resnet50,candidate_loader,'candidate')\n",
    "\"\"\" with torch.no_grad():\n",
    "    for i, (imgs, identityID, Ethnicity, faceID) in tqdm_notebook(enumerate(candidate_loader),total=len(candidate_loader)):\n",
    "        x = resnet50(imgs)\n",
    "        out = x.view(x.size(0),-1)\n",
    "        outputs.append(out)\n",
    "        identities.append(np.array(identityID))\n",
    "        ethnicities.append(np.array(Ethnicity))\n",
    "        faceIDs.append(np.array(faceID))\n",
    "\n",
    "outputs=torch.cat(outputs)\n",
    "identities= np.array(identities)\n",
    "ethnicities= np.array(ethnicities)\n",
    "faceIDs= np.array(faceIDs)\n",
    "\n",
    "torch.save(outputs, 'candidate_outputs.pt')\n",
    "np.save('candidate_identities.npy', identities)\n",
    "np.save('candidate_ethnicities.npy', ethnicities)\n",
    "np.save('candidate_faceIDs.npy', faceIDs) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# face verification (1:1)\n",
    "\n",
    "def face_verification(reference, candidate, metric, threshold=None):\n",
    "    ''' \n",
    "    this function performs face verification given a reference face and a candidate face\n",
    "    returns 0 if the faces do not match and 1 if they do\n",
    "    '''\n",
    "    if metric == 'correlation':\n",
    "        if threshold is None:\n",
    "            threshold = 0.8\n",
    "        cor = np.abs(corr2_coeff(reference,candidate))\n",
    "        return cor\n",
    "        \"\"\"if cor > threshold:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\"\"\"\n",
    "        \n",
    "    elif metric == 'cosine':\n",
    "        if threshold is None:\n",
    "            threshold = 0.5\n",
    "        cos = distance.cosine(reference,candidate)\n",
    "        if cos <= threshold:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    else:\n",
    "        return \"Please use one of 'correlation', or 'cosine' as an input for metric\"\n",
    "        \n",
    "def corr2_coeff(A, B):\n",
    "    # Rowwise mean of input arrays & subtract from input arrays themeselves\n",
    "    A_mA = A - A.mean(1)[:, None]\n",
    "    B_mB = B - B.mean(1)[:, None]\n",
    "\n",
    "    # Sum of squares across rows\n",
    "    ssA = (A_mA**2).sum(1)\n",
    "    ssB = (B_mB**2).sum(1)\n",
    "\n",
    "    # Finally get corr coeff\n",
    "    return torch.matmul(A_mA, B_mB.T) / torch.sqrt(torch.matmul(ssA[:, None],ssB[None]))\n",
    "def cos_sim(a, b, eps=1e-8):\n",
    "    \"\"\"\n",
    "    added eps for numerical stability\n",
    "    \"\"\"\n",
    "    a_n, b_n = a.norm(dim=1)[:, None], b.norm(dim=1)[:, None]\n",
    "    a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
    "    b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
    "    sim_mt = torch.mm(a_norm, b_norm.transpose(0, 1))\n",
    "    return sim_mt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11403, 2048]) (11403,) (11403,) (11403,)\n",
      "torch.Size([29117, 2048]) (29117,) (29117,) (29117,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['m.0c7mh2', 'm.0c7mh2', 'm.026tq86', ..., 'm.027nbyf', 'm.027nbyf',\n",
       "       'm.098d5s'], dtype='<U10')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for now since i didnt update the files i have to ravel here otherwise just load normally\n",
    "\n",
    "reference_outputs = torch.load(\"outputs/RFW/reference_outputs.pt\")\n",
    "reference_identities = np.concatenate(np.load('outputs/RFW/reference_identities.npy',allow_pickle=True)).ravel()\n",
    "reference_ethnicities = np.concatenate(np.load('outputs/RFW/reference_ethnicities.npy',allow_pickle=True)).ravel()\n",
    "reference_faceIDs = np.concatenate(np.load('outputs/RFW/reference_faceIDs.npy',allow_pickle=True)).ravel()\n",
    "print(reference_outputs.shape,reference_identities.shape,reference_ethnicities.shape,reference_faceIDs.shape)\n",
    "\n",
    "candidate_outputs = torch.load(\"outputs/RFW/candidate_outputs.pt\")\n",
    "candidate_identities = np.concatenate(np.load('outputs/RFW/candidate_identities.npy',allow_pickle=True)).ravel()\n",
    "candidate_ethnicities = np.concatenate(np.load('outputs/RFW/candidate_ethnicities.npy',allow_pickle=True)).ravel()\n",
    "candidate_faceIDs = np.concatenate(np.load('outputs/RFW/candidate_faceIDs.npy',allow_pickle=True)).ravel()\n",
    "print(candidate_outputs.shape,candidate_identities.shape,candidate_ethnicities.shape,candidate_faceIDs.shape)\n",
    "\n",
    "candidate_identities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor = corr2_coeff(reference_outputs,candidate_outputs).cpu().detach().numpy()\n",
    "cos = cos_sim(reference_outputs,candidate_outputs).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.7\n",
    "verification = pd.DataFrame(columns=['reference_identity','reference_ethnicity', 'cor_matches', 'cos_matches'])\n",
    "for i, (cor_row, cos_row) in tqdm_notebook(enumerate(zip(cor,cos)),total=len(cor)):\n",
    "  identity = reference_identities[i]\n",
    "  ethnicity = reference_ethnicities[i]\n",
    "  match_cor = candidate_identities[cor_row>thresh]\n",
    "  match_cos = candidate_identities[cos_row>thresh]  \n",
    "\n",
    "  row = {'reference_identity': identity,\n",
    "           'reference_ethnicity': ethnicity, \n",
    "           'cor_matches': match_cor, \n",
    "           'cos_matches': match_cos}\n",
    "  verification = verification.append(row,ignore_index=True)\n",
    "\n",
    "verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "372a96bfc8304b74acefb3c4d67e5409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11403 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference_identity</th>\n",
       "      <th>reference_ethnicity</th>\n",
       "      <th>TP</th>\n",
       "      <th>TN</th>\n",
       "      <th>FP</th>\n",
       "      <th>FN</th>\n",
       "      <th>matches</th>\n",
       "      <th>not_matches</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>324</th>\n",
       "      <td>m.0974m7</td>\n",
       "      <td>African</td>\n",
       "      <td>2</td>\n",
       "      <td>29115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[m.0974m7, m.0974m7]</td>\n",
       "      <td>[m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>406</th>\n",
       "      <td>m.0k00nk</td>\n",
       "      <td>African</td>\n",
       "      <td>3</td>\n",
       "      <td>29114</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[m.0k00nk, m.0k00nk, m.0k00nk]</td>\n",
       "      <td>[m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>m.01ky6fb</td>\n",
       "      <td>African</td>\n",
       "      <td>4</td>\n",
       "      <td>29113</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[m.01ky6fb, m.01ky6fb, m.01ky6fb, m.01ky6fb]</td>\n",
       "      <td>[m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>m.05zl3c</td>\n",
       "      <td>African</td>\n",
       "      <td>2</td>\n",
       "      <td>29115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[m.05zl3c, m.05zl3c]</td>\n",
       "      <td>[m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>m.02z21c6</td>\n",
       "      <td>African</td>\n",
       "      <td>2</td>\n",
       "      <td>29115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[m.02z21c6, m.02z21c6]</td>\n",
       "      <td>[m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11034</th>\n",
       "      <td>m.0h3ry9b</td>\n",
       "      <td>Indian</td>\n",
       "      <td>2</td>\n",
       "      <td>29115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[m.0h3ry9b, m.0h3ry9b]</td>\n",
       "      <td>[m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11037</th>\n",
       "      <td>m.0593ll</td>\n",
       "      <td>Indian</td>\n",
       "      <td>2</td>\n",
       "      <td>29115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[m.0593ll, m.0593ll]</td>\n",
       "      <td>[m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11195</th>\n",
       "      <td>m.04jb36f</td>\n",
       "      <td>Indian</td>\n",
       "      <td>2</td>\n",
       "      <td>29115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[m.04jb36f, m.04jb36f]</td>\n",
       "      <td>[m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11200</th>\n",
       "      <td>m.01yv6p</td>\n",
       "      <td>Indian</td>\n",
       "      <td>2</td>\n",
       "      <td>29115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[m.01yv6p, m.01yv6p]</td>\n",
       "      <td>[m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11308</th>\n",
       "      <td>m.0744sc</td>\n",
       "      <td>Indian</td>\n",
       "      <td>2</td>\n",
       "      <td>29115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[m.0744sc, m.0744sc]</td>\n",
       "      <td>[m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>280 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      reference_identity reference_ethnicity TP     TN FP FN  \\\n",
       "324             m.0974m7             African  2  29115  0  0   \n",
       "406             m.0k00nk             African  3  29114  0  0   \n",
       "441            m.01ky6fb             African  4  29113  0  0   \n",
       "542             m.05zl3c             African  2  29115  0  0   \n",
       "641            m.02z21c6             African  2  29115  0  0   \n",
       "...                  ...                 ... ..    ... .. ..   \n",
       "11034          m.0h3ry9b              Indian  2  29115  0  0   \n",
       "11037           m.0593ll              Indian  2  29115  0  0   \n",
       "11195          m.04jb36f              Indian  2  29115  0  0   \n",
       "11200           m.01yv6p              Indian  2  29115  0  0   \n",
       "11308           m.0744sc              Indian  2  29115  0  0   \n",
       "\n",
       "                                            matches  \\\n",
       "324                            [m.0974m7, m.0974m7]   \n",
       "406                  [m.0k00nk, m.0k00nk, m.0k00nk]   \n",
       "441    [m.01ky6fb, m.01ky6fb, m.01ky6fb, m.01ky6fb]   \n",
       "542                            [m.05zl3c, m.05zl3c]   \n",
       "641                          [m.02z21c6, m.02z21c6]   \n",
       "...                                             ...   \n",
       "11034                        [m.0h3ry9b, m.0h3ry9b]   \n",
       "11037                          [m.0593ll, m.0593ll]   \n",
       "11195                        [m.04jb36f, m.04jb36f]   \n",
       "11200                          [m.01yv6p, m.01yv6p]   \n",
       "11308                          [m.0744sc, m.0744sc]   \n",
       "\n",
       "                                             not_matches  \n",
       "324    [m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...  \n",
       "406    [m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...  \n",
       "441    [m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...  \n",
       "542    [m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...  \n",
       "641    [m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...  \n",
       "...                                                  ...  \n",
       "11034  [m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...  \n",
       "11037  [m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...  \n",
       "11195  [m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...  \n",
       "11200  [m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...  \n",
       "11308  [m.0c7mh2, m.0c7mh2, m.026tq86, m.026tq86, m.0...  \n",
       "\n",
       "[280 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh = 0.65\n",
    "cos_verification = pd.DataFrame(columns=['reference_identity','reference_ethnicity','TP','TN','FP','FN','matches','not_matches'])\n",
    "for i, cos_row in tqdm_notebook(enumerate(cos),total=len(cos)):\n",
    "  identity = reference_identities[i]\n",
    "  ethnicity = reference_ethnicities[i]\n",
    "  matches = candidate_identities[cos_row>thresh]  \n",
    "  not_matches = candidate_identities[cos_row<=thresh]\n",
    "  TP = sum(matches == identity)\n",
    "  FP = sum(matches != identity)\n",
    "  TN = sum(not_matches != identity)\n",
    "  FN = sum(not_matches == identity)\n",
    "  \n",
    "\n",
    "  row = {'reference_identity': identity,\n",
    "           'reference_ethnicity': ethnicity, \n",
    "           'TP': TP,\n",
    "           'TN': TN,\n",
    "           'FP': FP,\n",
    "           'FN': FN,\n",
    "           'matches': matches, \n",
    "           'not_matches': not_matches}\n",
    "  cos_verification = cos_verification.append(row,ignore_index=True)\n",
    "\n",
    "cos_verification[cos_verification.FN == 0][cos_verification.FP == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reference_identity     m.0c7mh2m.026tq86m.02wz3ncm.0c012t4m.0p8s_gxm....\n",
       "reference_ethnicity    AfricanAfricanAfricanAfricanAfricanAfricanAfri...\n",
       "TP                                                                  4728\n",
       "TN                                                              86693849\n",
       "FP                                                                125651\n",
       "FN                                                                  2666\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "African_cos = cos_verification.loc[cos_verification.reference_ethnicity == 'African']\n",
    "African_cos.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reference_identity     m.0281bfhm.0421bfhm.02r80dqm.06w18lbm.02tcmtm....\n",
       "reference_ethnicity    AsianAsianAsianAsianAsianAsianAsianAsianAsianA...\n",
       "TP                                                                  4613\n",
       "TN                                                              72371880\n",
       "FP                                                                 93146\n",
       "FN                                                                  2574\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Asian_cos = cos_verification.loc[cos_verification.reference_ethnicity == 'Asian']\n",
    "Asian_cos.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reference_identity     m.05zdk2m.04064_hm.06414fsm.0dtsglm.02ww2f6m.0...\n",
       "reference_ethnicity    IndianIndianIndianIndianIndianIndianIndianIndi...\n",
       "TP                                                                  4944\n",
       "TN                                                              86531724\n",
       "FP                                                                 54936\n",
       "FN                                                                  2354\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Indian_cos = cos_verification.loc[cos_verification.reference_ethnicity == 'Indian']\n",
    "Indian_cos.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reference_identity     m.0cqh0qm.02r6ydbm.0415yw4m.049pq8m.03cdg6lm.0...\n",
       "reference_ethnicity    CaucasianCaucasianCaucasianCaucasianCaucasianC...\n",
       "TP                                                                  3914\n",
       "TN                                                              86110744\n",
       "FP                                                                 10104\n",
       "FN                                                                  3324\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Caucasian_cos = cos_verification.loc[cos_verification.reference_ethnicity == 'Caucasian']\n",
    "Caucasian_cos.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = 0.65\n",
    "cor_verification = pd.DataFrame(columns=['reference_identity','reference_ethnicity','TP','TN','FP','FN','matches','not_matches'])\n",
    "for i, cor_row in tqdm_notebook(enumerate(cor),total=len(cor)):\n",
    "  identity = reference_identities[i]\n",
    "  ethnicity = reference_ethnicities[i]\n",
    "  matches = candidate_identities[cor_row>thresh]  \n",
    "  not_matches = candidate_identities[cor_row<=thresh]\n",
    "  TP = sum(matches == identity)\n",
    "  FP = sum(matches != identity)\n",
    "  TN = sum(not_matches != identity)\n",
    "  FN = sum(not_matches == identity)\n",
    "  \n",
    "\n",
    "  row = {'reference_identity': identity,\n",
    "           'reference_ethnicity': ethnicity, \n",
    "           'TP': TP,\n",
    "           'TN': TN,\n",
    "           'FP': FP,\n",
    "           'FN': FN,\n",
    "           'matches': matches, \n",
    "           'not_matches': not_matches}\n",
    "  cor_verification = cor_verification.append(row,ignore_index=True)\n",
    "\n",
    "cor_verification[cor_verification.FN == 0][cor_verification.FP == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "African_cor = cor_verification.loc[cor_verification.reference_ethnicity == 'African']\n",
    "African_cor.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Asian_cor = cor_verification.loc[cor_verification.reference_ethnicity == 'Asian']\n",
    "Asian_cor.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Indian_cor = cor_verification.loc[cor_verification.reference_ethnicity == 'Indian']\n",
    "Indian_cor.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Caucasian_cor = cor_verification.loc[cor_verification.reference_ethnicity == 'Caucasian']\n",
    "Caucasian_cor.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dissEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ab01ac27ec76bffb5c2aa0a183c02b22b22d6a3289db72f56ae87ad680cea4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
